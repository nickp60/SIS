import argparse
import os
import re
import sys
import shutil
import subprocess
import itertools
import logging
import io
from contextlib import redirect_stdout


from .sis import main as sis_main
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord

# For debugging use only:
# def print(x):
#     sys.stderr.write(str(x) + "\n")

def get_minimal_logger(verbosity):
    """
    """
    logger = logging.getLogger(__name__)
    if (verbosity * 10) not in range(10, 60, 10):
        raise ValueError('Invalid log level: %s' % verbosity)
    # logging.basicConfig(level=logging.DEBUG)
    logger.setLevel(logging.DEBUG)
    # create console handler and set level to given verbosity
    console_err = logging.StreamHandler(sys.stderr)
    console_err.setLevel(level=(verbosity * 10))
    console_err_format = logging.Formatter("%(levelname)7s - %(message)s")
    console_err.setFormatter(console_err_format)
    logger.addHandler(console_err)
    logger.debug("Initializing logger")
    logger.debug("logging at level {0}".format(verbosity))
    return logger


def get_args():
    parser = argparse.ArgumentParser(
        description="Use nucmer's coords and SIS to generate to scaffold"+
        "some contigs against a reference, filling the baps with N's")
    parser.add_argument( "reference", action="store", type=str,
                         help="reference strain")
    parser.add_argument("contigs", action="store",
                        help="path to your contigs")
    parser.add_argument("-b", "--blend", action="store", dest="blend",
                        default=1000,
                        help="if two hits from the same contigs are " +
                        "separated by a gap smaller than this, the hits " +
                        "will be merged; default: %(default)s")
    parser.add_argument("-m", "--minimum", action="store", dest="minimum",
                        default=2000,
                        help="minimum length nucmer hit to consider; " +
                        "default: %(default)s")
    parser.add_argument("-s", "--similarity", action="store",
                        dest="similarity",
                        default=50,
                        help="when a contig has two hits that share " +
                        "similare start or end coordinates, the shorter hit" +
                        "will be filtered out to keep the longer hit; " +
                        "default: %(default)s")
    parser.add_argument("-u", "--unplaced", action="store_true",
                        dest="unplaced",
                        help="output unplaced contigs to stdout instead of " +
                        "the scaffolds")
    parser.add_argument("-t", "--thresh", action="store", dest="threshold",
                        help="Ignore nucmer hits with less percent identity " +
                        "than this; %(default)s,",
                        default=95)
    parser.add_argument("-v", "--verbosity", action="store", dest="verbosity",
                        help="logging verbosity output to stderr",
                        choices=[1,2,3,4,5],
                        type=int,
                        default=2)
    args = parser.parse_args()
    return(args)


def check_exes():
    """Ensure that needed system executables are present
    """
    program_dict = {"nucmer":None,
                    "show-coords":None,
                    "delta-filter":None}
    for program in program_dict.keys():
        if shutil.which(program) is not None:
            program_dict[program] = shutil.which(program)
        else:
            sys.stderr.write(
                str("%s must be in your PATH!  Exiting...\n") % program)
            sys.exit(1)
    return program_dict


def log_coords(msg, coords, logger):
    """ takes some of the pain out of logging nested lists
    """
    logger.debug("%s:\n %s",
                 msg,
                 "\n".join(
                     [y  for y in \
                      ["\t".join(
                          [str(x) for x in sublist]) for sublist in coords]]
                 )
    )


def make_nucmer_delta_show_cmds(exes, ref, query, out_dir, prefix="out", header=True):
    """construct MUMmer commands needed by SIS
    returns: results list of cmds
    """
    # nucmer
    nucmer_cmd = "{0} {1} {2} -p {3}/{4} > {3}/nucmer.log 2>&1".format(
        exes['nucmer'], ref, query, out_dir, prefix)
    # delta-filter
    delta_filter_cmd = \
        "{0} -1 {1}/{2}.delta 2> {1}/delta-filter.log > {1}/{2}.filter".format(
            exes['delta-filter'], out_dir, prefix)
    #show-coords
    show_coords_cmd = \
        "{0} {3}{1}/{2}.filter 2> {1}/show-coords.log > {1}/{2}.coords".format(
        exes['show-coords'], out_dir, prefix,
            "" if header else "-H ")
    return [nucmer_cmd, delta_filter_cmd, show_coords_cmd]


def parse_coords(coords_filename, min_length, thresh):
    """Parse the coordinate file generated by nucmer or promer
    NOTE:  We rely on SIS to call whether things should be on dofferent
    chromasomes;  thats why we dont return them
    """
    info_array  = []
    coords_file = open(coords_filename)
    for line in coords_file:
        match =  re.match("\s+(\d+)\s+(\d+)\s+\|\s+(\d+)\s+(\d+)\s+\|\s+(\d+)\s+(\S+)\s+\|\s+(\S+)\s+\|(.*)\s(\S+)", line)
        #                     s1        e1           s2      e2           len1   en 2         percent
        if match and match.group(1) != "[S1]" : #ignore header line
            s1     = int(match.group(1))
            e1     = int(match.group(2))
            s2     = int(match.group(3))
            e2     = int(match.group(4))
            len1   = int(match.group(5))
            len2   = int(match.group(6))
            idy    = float(match.group(7))
            extra  = match.group(8)
            contig = match.group(9)
            if idy > thresh and abs(e2 - s2) > min_length:
                info_array.append([s1,e1,s2,e2,contig])
    coords_file.close()
    info_array.sort()
    return info_array


def parse_sis(sis_file):
    """ parse SIS output
    returns a dict {scaff_A: [contig_1, contig2, ...]}
    """
    scaffolds     = {}
    line_count    = 0
    this_scaff    = None
    for line in sis_file.split("\n") :
        line_count = line_count + 1
        if line.rstrip() :
            match = re.match("^>(.*?)$", line)
            if match :
                this_scaff = match.group(1).rstrip().lstrip()
                scaffolds[this_scaff] = []
            else :
                match = re.match("^(.*) ([01])$", line)
                if match :
                    scaffolds[this_scaff].append(
                        [match.group(1),int(match.group(2))])
                else :
                    sys.stderr.write("Warning: ignoring line %s:\n %s\n" %
                                     (line_count, line))
    return scaffolds


def blend_close_hits(coords, blend, logger):
    """ remove gaps potentially introduced by indels between ref and contigs
    returns a coord list
    """
    new_coords = []
    prev_line = None
    logger.info("Blending overlapping or close hits")
    for line in coords:
        if prev_line is None:
            prev_line = line   # first time through
        else:
            # if next sequence is not from same contig, add the previous line
            if not line[4] == prev_line[4]:
                new_coords.append(prev_line)
                # reject if there is a mid-contig inversion
            #  if one line is 1 .. 60 and the next one is 100..60, than there
            # has been an inversion, and we cant simplify
            elif (prev_line[3] > prev_line[2] and line[3] < line[2] ) or \
                 (prev_line[2] > prev_line[3] and line[2] < line[3] ):
                 new_coords.append(prev_line)
            # check if the coords are close enoguh to merge
            elif abs(line[2] - prev_line[3]) < blend:
                logger.debug("Blending close neighboring hits: %s %s"%
                             (prev_line, line))
                prev_line[1] = line[1]  # change subject end
                prev_line[3] = line[3]  # change query/contig end
                line = prev_line #  by not appending, we re-process this line
                logger.debug("into: %s" % prev_line)
            # detect overlapping hits:  same contig, but the end of the
            #   previous hit is ahead of the start of this hit
            elif prev_line[1] > line[0]:
                logger.debug("Blending overlapping hits: %s %s"% (prev_line, line))
                prev_line[1] = line[1]  # change subject end
                prev_line[3] = line[3]  # change query/contig end
                line = prev_line  # by not appending, we re-process this line
                logger.debug("into: %s" % prev_line)
            else:
                new_coords.append(prev_line)
            prev_line = line
    # include the last line
    new_coords.append(prev_line)
    # log results
    log_coords(logger=logger, msg="Blended Coords", coords=new_coords)
    return new_coords


def filter_list(lst, pos1, pos2, sim, depth=0, logger=None):
    """ recursively remove the longest of similar matches
    Consider [[1,   300],
              [40,  298],
              [150, 300]]
    where pos1 = 0 and pos2 = 1
    """
    if depth == 0:
        logger.info("Filtering list to select the longest of multiple hits")
    # for each possible combination of the list
    #@ (0, 1), (1,2), (0,2)
    logger.debug("Recursion Depth: %d", depth)
    log_coords(logger=logger, msg="Old Coord List", coords=new_coords)
    for i1, i2 in itertools.combinations(
            list(range(0, len(lst))), 2):
        # if the differences between starts pass our threshold...
        #@ if abs(1 - 40) < 50: (yes) (ie similar start coordinates)
        start , end = pos1, pos2
        # check the starts for similar hits...
        logger.debug("(%d - %d)  < %d ?", lst[i1][pos1], lst[i2][pos1], sim)
        if abs(lst[i1][pos1] - lst[i2][pos1]) < sim:
            logger.debug("Yes, removing one of the hits")
            # pick the longer of the pair, removing the shorter in the new list
            #@ if abs(300-1) > abs(289 - 40) : (yes)
            if abs(lst[i1][pos2] - lst[i1][pos1]) > abs(lst[i2][pos2] - lst[i2][pos1]):
                new_lst = [x for x in lst if x != lst[i2]]
            else:
                new_lst = [x for x in lst if x != lst[i1]]
            logger.debug("Removed a hit with a similar start coord")
            return filter_list(new_lst, pos1, pos2, sim,
                               depth=depth + 1, logger=logger) # recurse
        else:
            logger.debug("No")
        # check the ends for similar hits...
        logger.debug("(%d - %d)  < %d ?", lst[i1][pos2], lst[i2][pos2], sim)
        if abs(lst[i1][pos2] - lst[i2][pos2]) < sim:
            logger.debug("Yes, removing one of the hits")
            # pick the longer:
            #@ if abs(300-1) > abs(289 - 40) : (yes)
            if abs(lst[i1][pos2] - lst[i1][pos1]) > abs(lst[i2][pos2] - lst[i2][pos1]):
                new_lst = [x for x in lst if x != lst[i2]]
            else:
                new_lst = [x for x in lst if x != lst[i1]]
            logger.debug("Removed a hit with a similar end coord")
            return filter_list(new_lst, pos1, pos2, sim,
                               depth=depth + 1, logger=logger) # recurse
        else:
            logger.debug("No")
    if len(lst) > 1:
        logger.info("Finished filtering close hits; but contig still " +
                    "has more than one hit. Selecting the longest")
        len_list = [abs(x[pos2] - x[pos1]) for x in lst]
        for i, l in enumerate(len_list):
            if l == max(len_list):
                return [lst[i]]
    else:
        return lst


def condensce_coords(coords, sim=50, blend=0, logger=None):
    """ simplify coords file to only include contig boundaries

    SIS does not break contigs, neither does this
    it is common to have multiple hits for each contig, indicating indels.
    Here, we filter out the internal hits, only keeping the ones neccessary
    to calculate the gaps between contigs
    First, if two hits are separated by the --blend, combine into a single hit
    Then, similar hits exist (within -/+ sim), take the longer one.
    Then, remove any internal hits

    OLD:
    [S1]     [E1]  |   [S2]     [E2]  | [LEN 1]  [LEN 2]  | [% IDY]  | [TAGS]
==============================================================================
       3    65371  |  65375        1  |  65369    65375  |  99.91  | c_g NODE_1
   70474    80282  |      1     9809  |   9809     9809  | 100.00  | c_g NODE_3
   80326    85557  |   9888    15119  |   5232     5232  |  99.90  | c_g NODE_2
   85444   100560  |      1    15117  |  15117    15117  |  99.99  | c_g NODE_2
  100675   105547  |  15638    20510  |   4873     4873  | 100.00  | c_g NODE_2
  100558   100751  |      1      194  |    194      194  | 100.00  | c_g NODE_4

    NEW:
    [S1]     [E1]  |   [S2]     [E2]  | [LEN 1]  [LEN 2]  | [% IDY]  | [TAGS]
===============================================================================
       3    65371  |  65375        1  |  65369    65375  |  99.91  | c_g NODE_1
   70474    80282  |      1     9809  |   9809     9809  | 100.00  | c_g NODE_3
   80326    105547 |   9888    15119  |   5232     5232  |  99.90  | c_g NODE_2
  100558   100751  |      1      194  |    194      194  | 100.00  | c_g NODE_4

    NOTE:  we assume ordering at this stage, where coordinates of the
           reference in the first column [S1] are ascending
    """
    coords = blend_close_hits(coords=coords, blend=blend, logger=logger)
    processed_contigs = []
    filtered_coords = []
    new_coords = []
    all_contigs = [line[4] for line in coords]
    duplicated_contigs = list(set([x for x in all_contigs if all_contigs.count(x) > 1]))
    logger.debug("duplicated_contigs: %s", " ".join(duplicated_contigs))
    # select best hits when overlapping
    for contig in set(all_contigs):
        contig_coords_list = [x for x in coords if x[4] == contig]
        if contig in duplicated_contigs:
            contig_coords_list = filter_list(contig_coords_list, 2,3, sim)
        filtered_coords.extend(contig_coords_list)
    # condensce
    log_coords(logger=logger, msg="Filtered Coord List",
               coords=filtered_coords)
    for idx, line in enumerate(filtered_coords):
        if line[4] not in processed_contigs:
            new_coords.append(line)
            processed_contigs.append(line[4])
        else: # if contig already found, warn and ignore
            logger.warning(str(
                "Warning: %s found twice in the filtered coordinate list, " +
                "likely indicating major rearrangements. Check a visualization " +
                "of the contigs against your reference. only outputting first hit"), line[4])
    new_coords.sort()
    log_coords(logger=logger, msg="SORTED coords", coords=new_coords)

    return new_coords


def write_scaffold(coords, contig_file, thresh, sis_scaff, logger):
    """ for each contig in coords, read through the contigs, find the rec, and write it out
    this is probably very wasteful, but oh well
    returns  number of N's and total length
    """
    results = {"Scaffold name": [],
               "N's": [],
               "Length": []
    }
    scaff_number = 1
    for scaffold, contigs in sis_scaff.items():
        rec_ids = []
        new_seq = ""
        Ns = 0
        scaffolded_sequences = []
        for contig_ori_pair in contigs:
            scaffolded_sequences.append(contig_ori_pair[0])
        for idx, line in enumerate(coords):
            # skip entries not found in this scaffold
            if line[4] not in scaffolded_sequences:
                logger.debug("Not writing out %s in scaffold %d" % line[4], scaff_number)
                continue
            logger.debug("processing " + " ".join([str(x) for x in line]))
            FOUND = False
            with open(contig_file, "r") as inf:
                for rec in SeqIO.parse(inf, "fasta"):
                    if idx == 0:  # first time through. populate the recs list
                        rec_ids.append(rec.id)
                    if rec.id == line[4].strip():
                        FOUND = True
                        if line[2] < line[3]:  # if forward
                            new_seq = new_seq + rec.seq
                        else:  # if reverse, get the reverse complememnt
                            new_seq = new_seq + rec.reverse_complement().seq
                        #  if not the last entry, calculate the gap length
                        if idx != len(coords) - 1:
                            #                 next start - this end
                            gap_len = coords[idx + 1][0] - line[1]
                            # we ignore negative gaps, which would indicate
                            # a contig insertion
                            if gap_len < 0:
                                gap_len = 0
                            logger.debug("adding gap of %d" % gap_len)
                            new_seq = new_seq + ("N" * gap_len)
                            Ns = Ns + gap_len
                if not FOUND:
                    logger.error(
                        str("Contig %s found in coords file but " +
                            "not in contigs file seq ids: \n%s! Exiting...\n") %\
                        (line[4], "\n".join(rec_ids)))
                    sys.exit(1)
        results['Scaffold name'].append("SIS_Scaffold_%05d" % scaff_number)
        results["N's"].append(Ns)
        results['Length'].append(len(new_seq))
        sys.stdout.write(SeqRecord(new_seq, id="SIS_Scaffold_%05d" % scaff_number).format("fasta"))
        scaff_number = scaff_number +  1
    return (results)


def write_unplaced(coords, contig_file, thresh, sis_scaff, logger):
    """ for each contig, check if it is part of a scaffold.  if not, write it
    This check to see if the contig is either (a) in the filtered coords file or
    (b) in the scaffolded sequences file.  this gets rid of sequences
    not scaffolded by sis, or filtered out here due to lenght
    """
    unscaffolded_count = 0
    scaffolded_sequences = []
    coord_sequences = [x[4] for x in coords]
    for scaff, lst in sis_scaff.items():
        for pair in lst:
            scaffolded_sequences.append(pair[0])
    unscaffolded_seqs = []
    with open(contig_file, "r") as inf:
        for rec in SeqIO.parse(inf, "fasta"):
            if rec.id not in scaffolded_sequences or rec.id not in coord_sequences:
                sys.stdout.write(rec.format("fasta"))
                unscaffolded_count = unscaffolded_count + 1
    return unscaffolded_count


def main(args=None):
    if args is None:
        args = get_args()
    exe_dict = check_exes()
    logger = get_minimal_logger(args.verbosity)
    cmds = make_nucmer_delta_show_cmds(
        exes=exe_dict, ref=args.reference, query=args.contigs,
        out_dir=os.getcwd(), prefix="out", header=True)
    logger.info("Running nucmer, delta-filter, and show-coords")
    for cmd in cmds:
        logger.debug(cmd)
        subprocess.run(cmd,
                       shell=sys.platform != "win32",
                       stdout=subprocess.PIPE,
                       stderr=subprocess.PIPE,
                       check=True)
    f = io.StringIO()
    logger.info("Generating scaffold with  SIS")
    with redirect_stdout(f):
        sis_main(args=["sis.py", os.path.join(os.getcwd(), "out.coords")])
    sis_out = f.getvalue()
    logger.debug("SIS results: \n%s", sis_out)
    logger.info("Parsing nucmer coordinates")
    coords = parse_coords(os.path.join(os.getcwd(), "out.coords"),
                          min_length=args.minimum,
                          thresh=args.threshold)
    log_coords(logger=logger, msg="Initial Coords", coords=coords)
    new_coords = condensce_coords(coords, sim=args.similarity, blend=args.blend, logger=logger)
    log_coords(logger=logger, msg="Condensced coords", coords=new_coords)
    scaffolds = parse_sis(sis_file=sis_out)
    if args.unplaced:
        unscaffolded_count = write_unplaced(
            new_coords, args.contigs,
            thresh=args.threshold, sis_scaff=scaffolds, logger=logger)
        logger.info("Finished! Wrote out %d unscaffolded contigs", unscaffolded_count)
    else:
        results = write_scaffold(
            new_coords, args.contigs, thresh=args.threshold,
            sis_scaff=scaffolds, logger=logger)
        logger.info("Finished! Wrote out %d scaffold(s)", len(results['Scaffold name']))
        sys.stderr.write("Name\tLength\tN's\n")
        for i in range(len(results['Scaffold name'])):
            sys.stderr.write("\t".join([
                results['Scaffold name'][i],
                str(results['Length'][i]),
                str(results["N's"][i])
                ]) + "\n")
    return 0
